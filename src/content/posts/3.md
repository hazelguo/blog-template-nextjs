---
title: "More Random Sample"
date: "2025-05-21"
author: "Hazel Guo"
authorTitle: "CTO"
readTime: 10
excerpt: "Why Human Review Still Matters in High-Performing AI: Lessons From Liquor Matching"
image: "/covers/1.png"
---

## Introduction

Matching liquor products across different sources might sound straightforward, but it’s a surprisingly hard problem. In this project, we built an AI pipeline using large language models (LLMs) to automatically match liquor listings (e.g. products from various websites or databases) to a canonical catalog. The goal was to replace or augment the client’s in-house manual matching process. The challenge? Liquor products have a vast long-tail distribution of names and variants. Different retailers and sources describe the same bottle in diverse ways – from slight naming differences to major variations in descriptions – making it difficult to determine what’s a true match. It’s a classic entity resolution task complicated by the quirks of liquor naming, vintage, and branding. This is hard for humans and machines alike, but crucial for things like price comparison, inventory tracking, or search functionality. The context of this project was to see if an AI could do better than humans at liquor matching, and if so, how to handle the cases where it still falls short.

## Performance Comparison

The results so far show the AI system handily outperforms the client’s human-only pipeline on general liquor listings. The client’s original manual process achieved only about a 30% match rate (i.e. only ~30% of liquor entries were successfully matched to the catalog by their human team). In contrast, our LLM-based pipeline matched 350 out of 700 test entries (50%) on its own, immediately covering much more ground. With a human reviewer checking and refining the AI’s suggestions for those 700 entries, the matches rose to 435/700, about 62%. 

However, the picture changed when we looked at specialized liquors – the long-tail, niche products. Out of 2751 specialized items, the AI could confidently match only 700 (~25.5%). After bringing in an expert human reviewer to handle the remainder, the matches jumped to 1624/2751 (~59%). In other words, the AI alone struggled on more than two-thirds of these niche cases, but with human help we were able to cover well over half of them. This contrast between general vs. specialized liquors highlights how the AI excels with common products but stumbles on the rarer, more complex ones. 

Figure: Match rate comparison between the client’s original human-only pipeline and the new AI-driven pipeline, across general vs. specialized liquor categories, both before and after adding human review. The AI system achieved ~50% match coverage on general liquors (boosting to 62% with human review) and ~25% on specialized liquors (boosting to ~59% with expert human review), compared to ~30% for the previous human-only process. 

As the figure above illustrates, the AI-powered approach dramatically improved matching for the common liquors segment – reaching about double the coverage of the old human-only process. Even on the specialized liquors, the AI found many matches that the manual process likely missed. But it still left a large gap that only human expertise could fill. The takeaway is clear: AI can scale up and beat human performance on average, but there’s a long tail of tricky cases where human-in-the-loop intervention remains essential.

## Why AI Struggles with Specialized Cases

If the AI is so good with mainstream liquors, why does it fail on the specialized ones? It comes down to a few factors:
Description Variance Across Websites: The same liquor can be described very differently by different sellers or sources. Mainstream products (e.g. a popular whiskey or wine) often have standardized naming. But niche craft spirits, limited editions, or regional liquors might have inconsistent or incomplete descriptions. One site might list a whiskey as “John Doe Distillery Single Barrel 2019 – Special Release”, while another simply calls it “John Doe Single Barrel Special Release 2019” in a different format. An LLM looking at these might not immediately realize they refer to the same product if key words differ or information is missing. The AI’s understanding is only as good as the text it sees – and in specialized cases that text can be sparse or oddly formatted. This lack of uniform data makes it hard for the model to confidently say two entries are a match.
Long-Tail, Rare Knowledge: Large language models are trained on vast amounts of text, but they still have blind spots on very niche knowledge. Many of these specialized liquors are rare products that might not appear frequently in the AI’s training data. The “long tail” problem of AI is well documented: models see lots of examples of common scenarios, but very few of the rare ones, so they perform poorly on those edge cases
wp0.vanderbilt.edu
. In our case, the AI might have learned general patterns for matching common liquor names, but it hasn’t seen enough examples of that one obscure artisanal brand or a one-off limited release to recognize it across sources.
Strict vs. Loose Matching Tradeoff: Our AI pipeline is tuned to avoid false positives (incorrect matches) as much as possible. In product matching, a false positive could mean treating two different liquors as the same – a serious mistake that could mislead customers or corrupt data. To prevent that, the model and the matching rules are kept fairly strict: the AI only declares a match when it’s quite confident. This inevitably means some true matches are left on the table if the AI isn’t 100% sure – it favors precision over recall. We could loosen the criteria to catch more matches (improving recall), but that would risk erroneous matches. This precision–recall balancing act is tricky. For general liquors, the AI finds enough clear signals to match confidently. But for specialized liquors, the signals are often borderline or hidden in unstructured text, so the AI hesitates (as it should, to avoid mistakes). In other words, ensuring we “capture all relevant matches” is hard without also capturing some incorrect ones
dataweave.com
. The AI’s cautious approach means it misses many specialized matches that a human might have caught by reading between the lines.
Avoiding False Positives: It’s worth emphasizing why false positives are so heavily avoided. Imagine the system incorrectly matches a rare 18-year single malt scotch with a 12-year variant because the descriptions were vaguely similar. That kind of error could mislead inventory or confuse customers expecting one product and seeing it conflated with another. In the liquor domain, details like year, batch, or edition matter a lot. The cost of a bad match (false positive) is considered worse than a missed match (false negative). This conservative stance is another reason the AI will leave a specialized item unmatched rather than guess. Even small matching errors can have significant consequences in downstream decisions
dataweave.com
, so the model errs on the side of caution. The downside, of course, is reduced coverage on those edge cases.
Example: As a placeholder example, consider a niche product like a limited-release bourbon. One source lists it as “Maker’s Mark Private Select – Rickhouse Series, 2021 Edition” and another source has it as “2021 Maker’s Mark Private Select Rickhouse Release”. The AI might fail to match these because the wording order is different and one description omits a keyword like “Series” vs “Release”. A human with domain knowledge, however, would easily recognize that Maker’s Mark had a special Rickhouse series in 2021 and both entries refer to the same rare bourbon. In this case, the AI’s text-based understanding wasn’t enough – but a human reviewer can bridge the gap by using context and external knowledge. (In a real blog post, this is where we’d show an actual example from the data.) In summary, the AI struggles with specialized liquors due to data variability, lack of training examples for the long tail, and the intentional strictness of the matching criteria to avoid errors. These factors make the AI either unaware of some matches or too hesitant to commit to them. That’s where human experts come in.
The Value of Human-in-the-Loop
Introducing human-in-the-loop review proved to be the game-changer for those difficult cases. The idea is to let the AI do what it does best – rapidly processing and matching the obvious cases – and then have human experts handle the tricky bits. This hybrid approach combines the speed and consistency of AI with the judgment and expertise of humans. In our liquor matching project, the human reviewers focused on the unresolved and doubtful cases flagged by the AI, and their intervention raised the overall match coverage dramatically (as seen with the specialized liquors going from 700 to 1624 matches). There are several key benefits to adding human-in-the-loop for these specialized scenarios:
Catching Edge Cases: Humans are very good at handling one-offs and edge cases. A specialized liquor that the AI has never seen before might stump the model, but a human can research it, recall domain knowledge, or use context clues to make the match. Edge cases often require understanding subtle cues (like a regional naming convention or a clue hidden in the description) that a person can pick up on. By having humans cover these, the system ensures coverage of the long tail that the AI alone would miss
wp0.vanderbilt.edu
.
Insurance Against False Positives: Human review acts as a safety net to verify the AI’s matches. If the AI is unsure or possibly matching something incorrectly, a human in the loop can double-check and veto a bad match. This provides an insurance policy of sorts – we don’t have to loosen the AI’s criteria and risk a bunch of false positives, because we know anything borderline will get a human eyeball on it. It’s a quality control step. In practice, the AI pipeline can mark certain matches as “low confidence” or hold them for review, and an expert will confirm if it’s a true match or not. This way, the final output maintains high precision. (As an added benefit, the human might catch an AI false positive and correct it before it causes any harm.)
Improving the AI via Feedback: The human-in-the-loop process isn’t just about fixing one-off cases – it’s also a learning opportunity for the AI. Each time a human reviewer identifies a match that the AI missed or corrects a mistake, that instance becomes training data for future model improvements. We log those corrections and can feed them back into model fine-tuning or prompt adjustments. Over time, the AI will get better at those specialized cases because it’s learning from the experts. As noted in one product matching study, having humans validate and correct AI suggestions creates an iterative feedback loop that pushes accuracy closer to 100% and enhances the model’s reliability in complex scenarios
dataweave.com
. In our case, the expert reviewers are essentially teaching the LLM about these niche liquors, expanding its knowledge. That means the long-tail problem becomes a bit less severe with each review cycle – today’s edge-case failure might be handled automatically in a future iteration of the model.
Maintaining High Accuracy and Coverage: By combining AI and human strengths, we achieve both scale and accuracy. The AI handles the bulk matching quickly (scaling to thousands of items), and humans ensure that the difficult 10-20% of cases are not left behind. This blend is often the only practical way to reach the high accuracy levels demanded in production. For example, an AI might get you to 80–90% accuracy on its own, but reaching that last mile (95%+ with comprehensive coverage) is “very challenging for AI alone” and usually requires human expertise in the loop
dataweave.com
dataweave.com
. In our results, we see that with human help we could roughly double the coverage on specialized liquors compared to AI alone – an outcome that purely automated methods would struggle to achieve without sacrificing precision. The human reviewers essentially ensure that no stone is left unturned, validating the AI’s output and extending it where needed.
Conclusion and Call to Action
This liquor matching project taught us a valuable lesson: AI and humans together can outperform either one alone. The LLM-based system already blew past the manual matching rates in the general case, but its limitations on the long-tail specialized items became apparent. Rather than viewing that as a failure, we treated it as an opportunity to integrate human expertise right where the AI faltered. The result was a robust pipeline where AI does the heavy lifting and humans handle the exceptions – delivering superior overall performance and confidence in the results. For teams working with AI on problems that involve long-tail data, edge-case failures, or high accuracy requirements, it’s worth considering a similar human-in-the-loop approach. Don’t let the pursuit of full automation prevent you from achieving real-world reliability. Incorporating specialized human reviewers into your AI workflow can provide the best of both worlds: the efficiency and scale of automation with the nuance and judgment of human experts. It acts as both a safety net and a continuous improvement mechanism for your AI. In practical terms, this approach can mean the difference between a system that’s “mostly good” and one that truly meets your quality bar. If you’re tackling a problem where uncommon cases or subtle distinctions matter (whether it’s product matching, content moderation, medical data, etc.), think about integrating a human review step for those tricky cases. The long tail of data is where AI often struggles, but with a human-in-the-loop, you can conquer that long tail and keep your AI’s performance high and trustworthy. Interested in this human-plus-AI strategy? We invite teams facing these challenges to reach out or to experiment with adding a human-in-the-loop. You might be surprised at how much it boosts your AI system’s effectiveness, and how it accelerates learning for both the humans and the models. 
